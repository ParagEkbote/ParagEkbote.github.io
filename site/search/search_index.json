{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home \ud83c\udfe0     ML &amp; Data Science Engineer | Python | Research \u2192 Production Deployment | 1 year supporting tech startups | Passionate OSS Contributor              Welcome! Here, I showcase my journey of building and contributing to the ever-evolving world of          Artificial Intelligence.","text":""},{"location":"Education%20%26%20Work%20Experience/","title":"Education\ud83c\udf93 &amp; Work Experience\ud83d\udcbc","text":""},{"location":"Education%20%26%20Work%20Experience/#education","title":"Education","text":"<p>Bachelor of Technology in Artificial Intelligence &amp; Data Science </p> <p>Dr. D.Y. Patil Vidyapeeth \u2014 Pune, India (2023 \u2013 2027) </p>"},{"location":"Education%20%26%20Work%20Experience/#work-experience","title":"Work Experience","text":"<p>AI Research Intern \u2014 GreenPepper (Feb 2024 \u2013 Dec 2024) </p> <p>Key Contributions: </p> <ul> <li>Authored over five comprehensive narrative reports that effectively communicated complex business trends in Generative AI to key stakeholders, facilitating informed strategic decision-making</li> <li>Conducted thorough analyses of research papers, industry reports, and scholarly articles, extracting actionable insights to guide organizational strategy and innovation initiatives</li> <li>Led the development of \"The Generative AI Stack\", producing a detailed 20+ page technical guide designed to accelerate innovation in business applications and use cases</li> <li>Actively participated in the GenAI Sprint initiative, specializing in research of AI applications within the medical domain, contributing to breakthrough insights in healthcare technology</li> </ul> <p>Skills Developed: Research analysis, technical writing, stakeholder communication, AI development, consulting</p>"},{"location":"Open-Source%20Contributions/","title":"Open-Source Contributions\ud83d\udee0\ufe0f","text":"<p>I am an enthusiastic open-source contributor with over 80+ pull requests merged across a variety of well-established Machine Learning and AI repositories. </p> <p>My philosophy towards open source is rooted in giving back to the community and supporting accessible, scalable AI development. Each merged pull request represents an opportunity to learn, teach and strengthen tools that power cutting-edge applications worldwide.</p>"},{"location":"Open-Source%20Contributions/#notable-contributions","title":"\ud83c\udf1f Notable Contributions","text":"<ul> <li> <p>Optimized Models: torchao &amp; Pruna Quantization   Curated a collection showcasing optimized models using TorchAO and Pruna quantization for efficient deployment.</p> </li> <li> <p>Cooking with HF: My Recipe Contribution   Shared a practical recipe demonstrating Hugging Face workflows with Optuna for reproducible ML experimentation.</p> </li> <li> <p>Add Example of <code>IPAdapterScaleCutoffCallback</code> to Docs   Added an advanced inference callback example to improve user understanding of Diffusers pipelines.</p> </li> <li> <p>Notebooks for Diffuser Community Scripts   Contributed notebooks showcasing community scripts for streamlined diffusion model usage.</p> </li> <li> <p>Add an example using Optuna and Transformers   Demonstrated hyperparameter tuning of Transformer models with Optuna in the Optuna Examples Repo.</p> </li> <li> <p>Integrate <code>pyproject.toml</code> to Improve CI/CD and Tooling   Migrated Skorch to <code>pyproject.toml</code> for enhanced tooling compatibility and streamlined CI/CD processes.</p> </li> <li> <p>Add Example for Skorch DataLoader   Added a practical DataLoader usage example to improve onboarding for Skorch users.</p> </li> <li> <p>Add Example for Comet   Integrated a Comet logging example to enhance experiment tracking in Optuna workflows.</p> </li> <li> <p>Reduce core dependencies in pyproject.toml   Optimized core dependencies to improve installation efficiency and maintainability.</p> </li> </ul>"},{"location":"Projects/","title":"Projects \u2699\ufe0f","text":""},{"location":"Projects/#1-optimized-models-torchao-pruna-quantization","title":"1. Optimized Models: torchao &amp; Pruna Quantization","text":"<p>A collection of AI models quantized with torchao and Pruna to deliver faster inference and lower deployment cost.</p> <ul> <li> <p>Built a curated collection of 8 quantized AI models covering text-to-text, image-to-text, audio-to-text, and text-to-image modalities.</p> </li> <li> <p>Optimized each model with low-bit precision, enabling efficient inference while preserving strong performance across generative and understanding tasks.</p> </li> <li> <p>Reduced compute and memory overhead, making the models more practical for deployment in real-world applications.</p> </li> </ul> <p>Tech Stack: <code>PyTorch</code>, <code>torchao</code>, <code>transformers</code>, <code>diffusers</code>, <code>pruna</code>, <code>GPU acceleration</code>, <code>bitsandbytes</code></p>"},{"location":"Projects/#2-containerized-models-llms-diffusion-models-for-fast-reproducible-inference","title":"2. Containerized Models: LLMs &amp; Diffusion Models for Fast, Reproducible Inference","text":""},{"location":"Projects/#memory-efficient-small-language-model","title":"Memory-Efficient Small Language Model","text":"<ul> <li>Leverages <code>Pruna</code> for model quantization and <code>torch.compile</code> for graph-level optimizations.</li> <li>Significantly reduces memory usage and boosts inference speed.</li> <li>Ideal for memory-constrained or on-premise environments.</li> </ul>"},{"location":"Projects/#high-throughput-4-bit-small-language-model","title":"High-Throughput 4-bit Small Language Model","text":"<ul> <li>Implements 4-bit quantization for minimal RAM/GPU requirements.</li> <li>Integrates <code>Flash Attention 2</code> and <code>Triton</code> fused kernels for high-throughput inference on modern hardware.</li> </ul> <p>Tech Stack: <code>PyTorch</code>, <code>transformers</code>, <code>diffusers</code>, <code>Unsloth</code>, <code>LoRA</code>, <code>pruna</code>, <code>BitsAndBytes</code>, <code>Flash Attention 2</code>, <code>torch.compile</code>, <code>Docker</code>, <code>Cog</code></p>"},{"location":"Projects/#3-flux-fast-lora-hotswap","title":"3. flux-fast-lora-hotswap","text":"<p>Deployed a optimized version of <code>FLUX.1-dev</code> with <code>LoRA</code>, a text-to-image model optimized for efficient and flexible serving.</p> <ul> <li> <p>Trigger-Based LoRA Hotswapping: Implemented automatic loading of style-specific LoRA adapters based on keywords in the prompt (e.g., \"ghibsky\"), enabling seamless style switching.</p> </li> <li> <p>4-bit Quantization (BitsAndBytes): Compressed model weights to 4-bit precision, reducing GPU memory usage while maintaining image quality.</p> </li> <li> <p>torch.compile Runtime Acceleration: Applied PyTorch runtime optimizations to cut down image generation latency and improve responsiveness.</p> </li> <li> <p>Impact: Achieved faster inference, lower memory footprint for text-to-image generation, thereby lowering overall computational cost.</p> </li> </ul> <p>Tech Stack: <code>PyTorch</code>, <code>transformers</code>, <code>diffusers</code>, <code>LoRA</code>, <code>BitsAndBytes</code>, <code>torch.compile</code>, <code>Cog</code></p>"}]}