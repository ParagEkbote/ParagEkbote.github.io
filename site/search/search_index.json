{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home \ud83c\udfe0     ML &amp; Data Science Engineer | Python | Research \u2192 Production Deployment | 1 year supporting tech startups | Passionate OSS Contributor              Welcome! Here, I showcase my journey of building and contributing to the ever-evolving world of          Artificial Intelligence.","text":""},{"location":"Education%20%26%20Work%20Experience/","title":"Education\ud83c\udf93 &amp; Work Experience\ud83d\udcbc","text":""},{"location":"Education%20%26%20Work%20Experience/#education","title":"Education","text":"<p>Bachelor of Technology in Artificial Intelligence &amp; Data Science </p> <p>Dr. D.Y. Patil Vidyapeeth \u2014 Pune, India (2023 \u2013 2027) </p>"},{"location":"Education%20%26%20Work%20Experience/#work-experience","title":"Work Experience","text":"<p>AI Research Intern \u2014 GreenPepper (Feb 2024 \u2013 Dec 2024) </p> <p>Key Contributions: </p> <ul> <li>Authored over five comprehensive narrative reports that effectively communicated complex business trends in Generative AI to key stakeholders, facilitating informed strategic decision-making</li> <li>Conducted thorough analyses of research papers, industry reports, and scholarly articles, extracting actionable insights to guide organizational strategy and innovation initiatives</li> <li>Led the development of \"The Generative AI Stack\", producing a detailed 20+ page technical guide designed to accelerate innovation in business applications and use cases</li> <li>Actively participated in the GenAI Sprint initiative, specializing in research of AI applications within the medical domain, contributing to breakthrough insights in healthcare technology</li> </ul> <p>Skills Developed: Research analysis, technical writing, stakeholder communication, AI development, consulting</p>"},{"location":"Open-Source%20Contributions/","title":"Open-Source Contributions\ud83d\udee0\ufe0f","text":"<p>I am an enthusiastic open-source contributor with over 80+ pull requests merged across a variety of well-established Machine Learning and AI repositories. </p> <p>My philosophy towards open source is rooted in giving back to the community and supporting accessible, scalable AI development. Each merged pull request represents an opportunity to learn, teach and strengthen tools that power cutting-edge applications worldwide.</p>"},{"location":"Open-Source%20Contributions/#notable-contributions","title":"\ud83c\udf1f Notable Contributions","text":"<ul> <li> <p>Optimized Models: torchao &amp; Pruna Quantization   Curated a collection showcasing optimized models using TorchAO and Pruna quantization for efficient deployment.</p> </li> <li> <p>Cooking with HF: My Recipe Contribution   Shared a practical recipe demonstrating Hugging Face workflows with Optuna for reproducible ML experimentation.</p> </li> <li> <p>Add Example of <code>IPAdapterScaleCutoffCallback</code> to Docs   Added an advanced inference callback example to improve user understanding of Diffusers pipelines.</p> </li> <li> <p>Notebooks for Diffuser Community Scripts   Contributed notebooks showcasing community scripts for streamlined diffusion model usage.</p> </li> <li> <p>Add an example using Optuna and Transformers   Demonstrated hyperparameter tuning of Transformer models with Optuna in the Optuna Examples Repo.</p> </li> <li> <p>Integrate <code>pyproject.toml</code> to Improve CI/CD and Tooling   Migrated Skorch to <code>pyproject.toml</code> for enhanced tooling compatibility and streamlined CI/CD processes.</p> </li> <li> <p>Add Example for Skorch DataLoader   Added a practical DataLoader usage example to improve onboarding for Skorch users.</p> </li> <li> <p>Add Example for Comet   Integrated a Comet logging example to enhance experiment tracking in Optuna workflows.</p> </li> <li> <p>Reduce core dependencies in pyproject.toml   Optimized core dependencies to improve installation efficiency and maintainability.</p> </li> </ul>"},{"location":"Projects/","title":"Projects \u2699\ufe0f","text":""},{"location":"Projects/#1-hf-docker-scalable-inference-with-quantized-ai-models","title":"1. HF \u00d7 Docker: Scalable Inference with Quantized AI Models","text":""},{"location":"Projects/#optimized-models-torchao-pruna-quantization","title":"\ud83d\udd17 Optimized Models: torchao &amp; Pruna Quantization","text":"<ol> <li> <p>Created a curated collection of 8 quantized AI models spanning text-to-text, image-to-text, audio-to-text, and text-to-image modalities.</p> </li> <li> <p>Each model is optimized for efficient inference using low-bit precision, while maintaining strong performance across diverse generative and understanding tasks.</p> </li> </ol> <p>Tech Stack: <code>PyTorch</code>, <code>torchao</code>, <code>transformers</code>, <code>diffusers</code>, <code>pruna</code>, <code>GPU acceleration</code></p>"},{"location":"Projects/#containerized-models-llms-diffusion-models-for-fast-reproducible-inference","title":"\ud83d\udd17 Containerized Models: LLMs &amp; Diffusion Models for Fast, Reproducible Inference","text":""},{"location":"Projects/#1-flux1-dev-model-serving","title":"1. FLUX.1-dev Model Serving","text":"<ul> <li> <p>Trigger-Based LoRA Hotswapping   Dynamically loads style-specific LoRA adapters based on prompt keywords (e.g., <code>\"ghibsky\"</code>).</p> </li> <li> <p>4-bit Quantization (<code>BitsAndBytes</code>)   Maintains output quality while drastically reducing memory usage.</p> </li> <li> <p><code>torch.compile</code> Acceleration   Reduces image generation latency via runtime graph optimization.</p> </li> </ul>"},{"location":"Projects/#2-memory-efficient-small-language-model","title":"2. Memory-Efficient Small Language Model","text":"<ul> <li>Leverages <code>Pruna</code> for model quantization and <code>torch.compile</code> for graph-level optimizations.</li> <li>Significantly reduces memory usage and boosts inference speed.</li> <li>Ideal for memory-constrained or on-premise environments.</li> </ul>"},{"location":"Projects/#3-high-throughput-4-bit-small-language-model","title":"3. High-Throughput 4-bit Small Language Model","text":"<ul> <li>Implements 4-bit quantization for minimal RAM/GPU requirements.</li> <li>Integrates <code>Flash Attention 2</code> and <code>Triton</code> fused kernels for high-throughput inference on modern hardware.</li> </ul> <p>Tech Stack: <code>PyTorch</code>, <code>transformers</code>, <code>diffusers</code>, <code>Unsloth</code>, <code>LoRA</code>, <code>pruna</code>, <code>BitsAndBytes</code>, <code>Flash Attention 2</code>, <code>torch.compile</code>, <code>Docker</code>, <code>Cog</code></p>"}]}